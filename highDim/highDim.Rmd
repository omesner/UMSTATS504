---
title: "High Dimensional Analysis"
author: "Octavio Mesner"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## Understanding Lymphoma

![Micrograph (Field stain) of a Diffuse Large B-Cell Lymphoma](https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Diffuse_large_B_cell_lymphoma_-_cytology_low_mag.jpg/300px-Diffuse_large_B_cell_lymphoma_-_cytology_low_mag.jpg)

From Wikipedia:

- Diffuse large B-cell lymphoma (DLBCL) is the most common lymphoid malignancy in adults

- Cancer of B cells, a type of lymphocyte

- Incidence: 7-8 cases per 100,000 people per year in the US

- occurs primarily in older individuals, with a median age of diagnosis at ~70 years

- it can occur in young adults and, in rare cases, children

- DLBCL can arise in virtually any part of the body and, depending on various factors, is often a very aggressive malignancy

- The causes of diffuse large B-cell lymphoma are not well understood

- Curable in < 50% of patients (in 2002)

- Here, we want to use gene expression or *microarray data* to predict DLBCL

- [Microarrays](https://en.wikipedia.org/wiki/Microarray) simultaneously detect the expression of thousands of genes from a sample

![Microarray Data](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Microarray2.gif/350px-Microarray2.gif)

- This [paper](https://idp.nature.com/authorize/casa?redirect_uri=https://www.nature.com/articles/nm0102-68&casa_token=vw6a-r_uXz8AAAAA:PBI84ZmryGuDsD_exYAi6aTypeES0fxnpFfASvUwbJ7o_Ihb5FsyW_IvxXGlrCj4UlIyydKIpRIbH4rx8A) use a supervised learning model on 6817 gene expressions (microarray) from 71 patients to distinguish between DLBCL and follicular lymphoma (FL)

- This data is on [Github](https://github.com/ramhiser/datamicroarray/wiki/Shipp-%282002%29)

- In this data, each row corresponds to an individual

- Each column corresponds to gene expression using a microarray, one column gives the type of lymphoma


```{r data}
load("./shipp.RData")
ls() # .RData files can include any R data structure.  The "ls()" command shows what it contains.
names(shipp)
dim(shipp$x)
names(shipp$x)[1:10]  #first 10 var names
shipp$x[1:10,1:6]
table(shipp$y)
```

- Note that there are far more columns than rows here
  - So standard regression methods will not work


- Question: Should we use a t-test to determine which genes are associated with DLBCL?
  - A: Yes
  - B: No
  - C: Not Sure

```{r naiveMultiple}
pvals <-c()
for(var in names(shipp$x)){
  pvals <- c(pvals, t.test(shipp$x[[var]] ~ shipp$y)$p.value)
}
hist(pvals)
sum(pvals < 0.05)
sum(pvals < 0.05)/length(pvals) # pct significant at alpha = 0.05
```


- Question: Do you believe that all tests with p < 0.05 are actually statistically significant?
  - A: Yes
  - B: No
  - C: Not Sure

**Multiple testing error**

- What would we get if none were significant?

- The data below are generated randomly and independent from the outcome
  - That is, we randomly generate data without using the outcome, so we know, theoretically that each column of generated data is independent from the outcome

```{r null}
set.seed(1234)
null_dat <- data.frame(matrix(rnorm(dim(shipp$x)[1] * dim(shipp$x)[2]), ncol=dim(shipp$x)[2]))
dim(null_dat)
null_pvals <-c()
for(var in names(null_dat)){
  null_pvals <- c(null_pvals, t.test(null_dat[[var]] ~ shipp$y)$p.value)
}
hist(null_pvals)
sum(null_pvals < 0.05)/length(null_pvals) # pct significant at alpha = 0.05
```


- Question: Did you expect that 5% of p-values < 0.05?
  - A: Yes
  - B: No
  - C: Not Sure

- Intuitively, what is the problem here?  Why are so many p-values $<0.05$ when none are associated with the outcome?

- Question: Did you expect the p-values to have a uniform distribution?
  - A: Yes
  - B: No
  - C: Not Sure


- Why do they look uniform?

**What is a hypothesis test/p-value?**

- Example:

  - Let $X_1, X_2,\dots, X_n$ be an independent, identically distributed sample 

  - Let $E[X_i]=\mu$ (unknown) and $E[(X_i-\mu)^2]=\sigma^2=1$ (known) for each $i=1,2,\dots,n$
  
  - Let $\bar X_n = \frac{1}{n}\sum_{i=1}^n X_i$
  - Is $\mu< 0$?
  - Let $H_0: \mu\geq0$ and $H_1: \mu< 0$
  - Intuitively, if $\frac{\bar X_n -\mu}{\sigma/\sqrt n} = \sqrt n \bar X$ is too big, then we can reject $H_0$
  - From the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), we know that $\frac{\bar X_n -\mu}{\sigma/\sqrt n} \leadsto N(0,1)$ (converges in distribution) as $n\rightarrow\infty$
  - That is, our test statistic, $T=\sqrt n \bar X$ is approximately $N(0,1)$ under $H_0$ (we're setting $\mu=0$)
  - p-value: $p\approx P[T<N(0,1)] = \Phi(T) = \int_{-\infty}^T \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}dx$

![p-value illustration](./pval.png)

[other illustration](http://blog.analytics-toolkit.com/2017/statistical-significance-ab-testing-complete-guide/)

**Theorem:** p-values are uniformly distributed under the null hypothesis.

*Proof:*

1. Let $T$ be a test statistic of a hypothesis test and let $F$ be the CDF of T under $H_0$, the null hypothesis

2. p-value: $p= F(T)$

3. Claim: $Y$ is uniformly distributed on $[0,1]$ $\Leftrightarrow$ $P[Y < y] = G(y) = y$ on $[0,1]$

4. Goal: show that $P[p < y] = y$ under the null hypothesis

5. Using $F^{-1}$, the inverse function of $F$,
\[
\begin{align*}
P[F(T) < y]
&= P[F^{-1}(F(T)) < F^{-1}(y)] \\
&= P(T < F^{-1}(y)) \\
&= F(F^{-1}(y)) \\
&= y
\end{align*}
\]

6. So, under the null hypothesis, p-value $\sim$Unif(0,1) $\blacksquare$

- Question: Did this proof make sense?
  - 1 (not at all) to 10 (perfect sense)

## Multiple Testing

- Want to run $m$ tests
- Let $H_{0,i}$ be the null hypothesis for the $i$th test where $1 \leq i \leq m$
- Let $p_i$ be the p-value of the $i$th test for $H_{0,i}$
- Let $R_i = 1$ if we reject $H_{0,i}$ and $R_i = 0$ if we fail to reject $H_{0,i}$ (sometimes called a discovery or positive result)
- $R = \sum_{i=1}^m R_i$ be the number of rejected tests
- Let $V_i = 1$ if we wrongly reject $H_{0,i}$ (false positive or type 1 error or false discovery)
- Let $V = \sum_{i=1}^m V_i$ be the false positive (discovery) count

- **Family-wise error rate (FWER)** 
\[P[V>0] \leq \alpha\]
  Same as $P[V=0] \geq 1-\alpha$
- **Per family error rate (PFER)**
  \[E[V] \leq \alpha\]
- **False discovery rate (FDR)** controls
\[E\left[\frac{V}{R}\right]\]
  - If $R=0$, use $R=1$ instead

- **Global null**

  - Can we reject at least one of the $m$ null hypotheses?
  - The global null hypothesis is
\[H_0 = \bigcap_{i=1}^m H_{0,i}\]
  - Does not indicate which $H_{0,i}$ to reject, only that we can reject at least one $H_{0,i}$

### Tests

**Bonferroni**

- Reject $H_{0,i}$ if $p_i \leq p_{\text{Bon}} := \frac{\alpha}{m}$
- uses FWER, tells us which $H_{0,i}$ we can reject

*proof*

Let $I := \{i: H_{0,i} = 1\}$ be the set of true null hypotheses.
\[
\begin{align}
P[V>0]
&= P\left[ \bigcup_{i\in I} \left\{ p_i \leq \frac{\alpha}{m} \right\} \right]\\
&\leq \sum_{i\in I} P\left[p_i \leq \frac{\alpha}{m}\right] & \text{Union bound}\\
&= \sum_{i\in I} \frac{\alpha}{m} &\text{because $p_i\sim$ Unif$(0,1)$ for $i\in I$}\\
&= \frac{\alpha |I|}{m} \leq \alpha
\end{align}
\]

More info on [union bound](https://en.wikipedia.org/wiki/Boole%27s_inequality)

- Think about playing cards: $P[\text{Ace OR club}] \leq P[\text{Ace}] + P[\text{club}]$
- Because of union bound, even holds when p-values are statistically dependent

Using Bonferroni, which gene's should I recommend are associated with lymphoma?

```{r bonferroni}
bcorrection <- 0.05/length(pvals)
bcorrection
sum(pvals <= bcorrection)
which(pvals <= bcorrection)
```

What about the independent simulated data?

```{r bcorsim}
which(null_pvals<=bcorrection)
```

- Notice that test may be associated (knowing one p-value may give information about others)
- Bonferroni still works here
- How could we construct a global null hypothesis from Bonferroni?
  - If at least one $p_i \leq \frac{\alpha}{m}$, then we can reject global null
- Bonferroni is conservative
  - Types of error: false positives (type I error) and false negatives (type II error)
  - Bonferroni is very good at limiting false positives but not limiting false negatives
  - Power = 1-P(type II error), is the probability of rejecting the null hypothesis when the alternative hypothesis is true
  - Bonferroni may not always be a powerful test

**Fisher Combination**

- Uses global null framework
- Assumes that each $p_i$ is independent (why might this not be reasonable in most settings?)
- If $H_{0,i}$ is true for each $i$ (and each test is independent), then
\[
T = \sum_{i=1}^m -2 \log(p_i) \sim \chi^2(2m)
\]
- uses that fact that transforming independent, uniform random variables this way will have a $\chi^2$ distribution
- This test does not indicate which $H_{0,i}$ to reject
- Why should we not use this test here?

**Simes Test**

- Uses global null framework
- Order all p-values, $p_{(1)}\leq p_{(2)}\leq \dots \leq p_{(m)}$ for the $m$ hypotheses
- Simes test statistic
\[ p_{\text{Simes}} = \min_{i\in [m]} \left\{ \frac{m p_{(i)}}{i} \right\}\]
- Reject global null if $p_{\text{simes}} \leq \alpha$
  - Equivalent: Reject global null if any $p_{(i)} \leq \frac{i\alpha}{m}$
- Proof is a little more complicated
  - Need to show that $p_{\text{Simes}} \sim \text{Unif}(0,1)$
  - Uses order statistics properties to show this
- Does not require all $p_i$ are independent
- More powerful (type II error is smaller) than Bonferroni global null test
  - Bonferroni will reject global null when $m p_{(1)} \leq \alpha$

```{r simes}
simes <- function(vec) length(vec)*min(sort(vec)/(1:length(vec)))

simes(pvals)
simes(null_pvals)
```

- Coding tip: DRY (don't repeat yourself)
- if you're tempted to copy your own code to reuse it, write a function for it instead

**Kolmogorov-Smirnov Test**

![image from wikipedia](./KS_Example.png)

- compared empirical cdf to theoretical cdf
  - Here: Assess fit of empirical p-value cumulative distribution compared to uniform
- Uses global null framework
- Empirical CDF of p-values is
\[\hat F_m(t) = \frac{1}{m} \sum_{i=1}^m I(p_i \leq t)\]
- Uniform CDF $F(t) = t$ for $t\in [0,1]$

```{r kst}
par(mfrow=c(1,2))
plot(ecdf(null_pvals), col='red', xlim=c(0,1), ylim=c(0,1), 
     main="Null Simulated")
abline(c(0,0), c(1,1))
plot(ecdf(pvals), col='red',
     main="Real p-values")
abline(c(0,0), c(1,1))
```

- Test statistic
\[T_m = \sup_{t\in [0,1]} \left| \hat F_m(t) - t \right|\]
- Using Hoeffding's inequality,
\[P\left[ T_m > t \right] \leq 2\exp\left(-2t^2\right)\]
- Reject global null if
\[T_m > \sqrt{\frac{2\log(\frac{2}{\alpha})}{2}}\]

```{r ksttest}
ks.test(pvals, runif(100000))
ks.test(null_pvals, runif(100000))
```

See [ks.test documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ks.test.html) for more info

**Benjamini-Hochberg**

- Method for controlling the false discovery rate (FDR)
- Interpretation: At an FDR of $\alpha$, we would expect, at most, $\alpha$ of our significant tests to be false positives

- Method:
  1. Order all p-values, $p_{(1)}\leq p_{(2)}\leq \dots \leq p_{(m)}$ for the $m$ hypotheses
  2. Let $j= \max\left\{i: p_{(i)} < \frac{i\alpha}{m}\right\}, T_{\text{BH}}=p_{(j)}$
  3. Reject $H_{0,i}$ for $p_i\leq T_{\text{BH}}$
- Alternative:
  - adjust p-values using
  \[\frac{m p_{(i)}}{i}\]

*Proof outline*

Recall
\[\text{FDR} = E\left[\frac{V}{R}\right] = E\left[\frac{\text{# Type 1 Error}}{\text{# Rejections}}\right].\]
Let $W_i=1$ if $H_{0,i}$ is true and $W_i=0$ otherwise.
Let $G(t)$ be the true CDF of the p-values and let $\hat G(t)$ be the empirical CDF as before.
\[
\begin{align}
\text{FDR} 
&= E\left[\frac{V}{R}\right]\\
&= E\left[\frac{\frac{1}{m}\sum_{i=1}^m W_i I(p_i < t)}{\frac{1}{m}\sum_{i=1}^m I(p_i < t)}\right]\\
&\approx \frac{E\left[ \frac{1}{m}\sum_{i=1}^m W_i I(p_i < t) \right]}{E\left[\frac{1}{m}\sum_{i=1}^m I(p_i < t)\right]}\\
&= \frac{t |I|}{G(t)} \leq \frac{t}{G(t)} \approx \frac{t}{\hat G(t)}
\end{align}
\]
Let $t=p_{(i)}$ for some $i$.
Notice that $\hat G(p_{(i)})=\frac{i}{m}$.
Then $\text{FDR}= \frac{p_{(i)} m}{i}$.
Setting this value equal to $\alpha$ and solving for $p_{(i)}$, we get the BH test statistic.

```{r bh}
bh_adj <- p.adjust(pvals, 'BH')
round(bh_adj, 3)[1:10]
round(pvals[1:10], 3)
which(bh_adj < 0.05)[1:10]
sum(bh_adj<0.05)
```

See [p.adjust documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/p.adjust.html) for more info

**Final thoughts on multiple testing**

- Family-wise error and false discovery
- Only Bonferroni and Benjamini-Hochberg indicate which null hypotheses to reject, all others are global null
- Many applied papers do not use multiple testing procedures but should
- Prediction, causation, and dependence
  - Smoking and cancer can both be used to predict death, but should we use both?

```{dot chain, echo=FALSE, out.width = '20%'}
digraph G {
  Smoking -> Cancer
  Cancer -> Death
}
```

- What about something more complex?
  - pink and red variables will be associated with Outcome
  - D and G are sufficient for prediction
  - if D and G are used, other variables provide no additional accuracy
  - regression takes this into account
  - $\beta = [\beta_A, \beta_B, \beta_C, \beta_D, \beta_E,\beta_F, \beta_G, \beta_H, \beta_Q, \beta_R,\beta_S, \beta_T] = [0,0,0,\beta_D,0,0, \beta_G, 0,0,0,0,0]$ where $\beta_D,\beta_G\neq 0$
  - $\{\beta_D,\beta_G\}$ is the *active* set
  - What should the active set be if $G$ were missing?

```{dot complex, echo=FALSE, out.width = '50%'}
digraph G {
  A -> D
  A -> E
  B -> E
  E -> G
  C -> G
  D -> Outcome
  F -> G
  F -> H
  G -> Outcome
  G -> H
  D [shape=circle, style=filled, fillcolor=red]
  G [shape=circle, style=filled, fillcolor=red]
  Outcome [shape=rectangle, style=filled, fillcolor=lightblue]
  A [shape=circle, style=filled, fillcolor=pink]
  B [shape=circle, style=filled, fillcolor=pink]
  C [shape=circle, style=filled, fillcolor=pink]
  E [shape=circle, style=filled, fillcolor=pink]
  F [shape=circle, style=filled, fillcolor=pink]
  H [shape=circle, style=filled, fillcolor=pink]
  Q -> R
  R -> S
  T -> R
}
```


## Penalized Regression

**Ordinary least squares**
\[\text{Have }
\mathbf{X} = \begin{bmatrix}
X_{11} & X_{12} & \cdots & X_{1p}\\
X_{21} & X_{22} & \cdots & X_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
X_{n1} & X_{n2} & \cdots & X_{np}\\
\end{bmatrix},
\mathbf{y}= \begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{bmatrix}.
\text{  Want to estimate }
\mathbf{\beta}= \begin{bmatrix}
\beta_1\\
\beta_2\\
\vdots\\
\beta_p
\end{bmatrix}
\]
so that 
\[\hat\beta = \text{arg} \min_\beta \left| \mathbf{y} - \mathbf{X}\mathbf{\beta}\right|\]

Using calculus, it's easy to show that
\[\hat \beta = \left(\mathbf X^T \mathbf X\right)^{-1} \mathbf X^T \mathbf y\]

OLS Pros:

- When true relationship between response and predictors is approximately linear, low bias
- When $n \gg p$, low variance

OLS Cons:

- When $n \not\gg p$, *overfitting* can be a problem
- $\left(\mathbf X^T \mathbf X\right)^{-1}$ only exists for $n >> p$.

**Ridge Regression**

\[\hat\beta = \text{arg} \min_\beta \frac{1}{n}\sum_{i=1}^n\left( y_i - X_i^T\mathbf{\beta}\right)^2 + \lambda\|\beta\|_2^2\]

Note: The Euclidean norm, $\|x\|_2=\sqrt{x_1^2+x_2^2+\cdots+x_p^2}$ for $x\in \mathbb{R}^p$

Similar to OLS, $\hat \beta$ also has a closed form
\[\hat \beta = \left(\mathbf X^T \mathbf X + \lambda I \right)^{-1} \mathbf X^T \mathbf y\]
where $I$ is the identity matrix

```{r ridge}
### Generating data
sample_size <- 50 # remember DRY
set.seed(1234)
num_active_vars <- 10
num_null_vars <- 20
true_beta <- 2*runif(num_active_vars) # randomly choosing true beta
true_beta
active_x <- matrix(rnorm(sample_size*num_active_vars), nrow=sample_size)
null_x <- apply(matrix(3*rnorm(sample_size*num_null_vars), nrow=sample_size), 2,
                function(x) x + 10*runif(1))
y <- active_x %*% true_beta + rnorm(sample_size)
dim(y) #sanity check
dat <- data.frame(cbind(active_x, null_x, y))
dim(dat)
names(dat)
names(dat)[31] <- 'Y' # renaming response variable
head(dat)

# install.packages("glmnet", repos = "http://cran.us.r-project.org") # only need 1st time
library(glmnet)
design_mat <- cbind(active_x, null_x) # glmnet only takes matrices, not dataframes
l2_fit <- glmnet(design_mat, y, family="gaussian", alpha=0) # alpha = 0 gives Ridge regression
plot(l2_fit, xvar='lambda', label=TRUE)
names(l2_fit)
coef(l2_fit)[1:10,1:5]
l2_fit$lambda

glmnet_plot <- function(fit, num_active){ #assumes active vars are first
  plot(0, type='n', ylim = range(coef(fit)[-1,]), xlim = log(range(fit$lambda)),
     ylab = "Coefficient", xlab="log(lambda)")
  num_vars <- dim(coef(fit))[1]-1 # removing intercept
  for(itr in 1:num_vars){
    active = c(rep('red', num_active), rep('gray', num_vars-num_active))
    lines(log(fit$lambda), coef(fit)[itr+1,], col=active[itr])
    legend('topright', legend = c('True Non-Zero', 'True Zero'), col=c('red', 'gray'), lty = 1)
  }
}
glmnet_plot(l2_fit, num_active_vars)
```

- As $\lambda$ gets bigger, coefficients shrink
- Notice that the coefficients get closer to zero but are never exactly zero
- Because of closed form, fast to compute

**LASSO**

\[\hat\beta = \text{arg} \min_\beta \frac{1}{n}\sum_{i=1}^n\left( y_i - X_i^T\mathbf{\beta}\right)^2 + \lambda\|\beta\|_1\]

- The $L_1$ norm or taxicab norm, $\|x\|_1=|x_1|+|x_2|+\cdots+|x_p|$ for $x\in \mathbb{R}^p$

![from wikipedia](./Manhattan_distance.png)


```{r lasso}
l1_fit <- glmnet(design_mat, y, family="gaussian", alpha=1) # alpha=1 gives lasso 
glmnet_plot(l1_fit, num_active_vars)
```

- $L_1$ penalty generates sparse $\beta$ (many zeros)
- As $\lambda$ gets bigger, $\beta$ shrinks and some go to zero
- Because of the absolute value, this function is not differentiable everywhere, standard optimization uses coordinate descent, also fast
- if $p>n$, lasso selects at most $n$ variables
- if using grouped (dummy) variables (like race or other categorical variables with more than 2 levles), lasso will ignore groups
- Can be erratic on collinear data

```{r lassoCollinear}
set.seed(1234)
dm_half <- design_mat[, c(1:5, 11:21)] # taking half of active and null columns
collin_vars <- dm_half+ 0.1*matrix(rnorm(dim(dm_half)[1]*dim(dm_half)[2]), nrow=sample_size) #collinear vars
collin_dm <- cbind(dm_half[,1:5], collin_vars[,1:5], dm_half[,6:16], collin_vars[,6:16])
l1_fit_collin <- glmnet(collin_dm, y, family="gaussian", alpha=1)
glmnet_plot(l1_fit_collin, num_active_vars)
```

**Difference between Ridge and LASSO**

- Why does the $L_1$-norm penalty in LASSO zero out some $\beta$ values but not for the $L_2$-norm penalty in Ridge?

![from Elements of Statistical Learning](./L2vsL1penalty.png)

- Think of XY-plane as all possible values of $\beta = (\beta_1,\beta_2)$
- Without the penalty $h(\beta_1,\beta_2) = \frac{1}{n}\sum_{i=1}^n (y_i-X_i^T\beta)^2$
  - because $h$ is quadratic, it forms a [parabolid](https://en.wikipedia.org/wiki/Paraboloid)
  - Want to find values of $(\beta_1,\beta_2)$ that minimize $h$
- $g(\beta_1,\beta_2) = \|\beta\|_1$ is an upside down pyramid with its bottom point at the origin
  - the level contours create empty diamond-shapes ($\{(x_1,x_2): \|(x_1,x_2)\|_1=z\}$ is a diamond-shape)
- By adding these two functions, the optimization must balance the contribution from each
  - $h$ wants the paraboid min and $g$ wants the origin
  - Because $g$ has a sharp point, the lowest contour of $h$ will likely hit there first
  
  ![like in this image](https://i.stack.imgur.com/UaoPh.png)

**Elastic Net**

\[\hat\beta = \text{arg} \min_\beta \frac{1}{n}\sum_{i=1}^n\left( y_i - X_i^T\mathbf{\beta}\right)^2 + \lambda\left[(1-\alpha)\|\beta\|_2^2/2 + \alpha|\beta\|_1\right]\]

```{r elastic}
l5_fit <- glmnet(design_mat, y, family="gaussian", alpha=0.5) 
glmnet_plot(l5_fit, num_active_vars)
```

![[image from here](https://corporatefinanceinstitute.com/resources/knowledge/other/elastic-net/)](./elastic.png)


- $L_1$ penalty generates sparse model
- $L_2$ penalty
  - number of selected variables not bounded by $n$
  - stabilizes $L_1$ regularization path
  
### Penalized Regression as Bayesian Method

- Bayes' Theorem $$P(H|E)=\frac{P(E|H) P(H)}{P(E)}$$ where $P(E)\neq 0$
- Note: Bayes' theorem is also frequently used outside of Bayesian methods
- In Bayesian inference, the goal is to estimate $P(H|E)$ by estimating $P(E|H)$ and $P(H)$
- $P(H|E)$, the *posterior probability*, is the probability $H$ given evidence (data) $E$
- $P(H)$, the *prior probability*, is the probability of $H$ before evidence (data) is observed
- $P(E|H)$ is the *likelihood*
- $P(E)$ is the marginal likelihood but is frequently used to make sure that the probability sums to one
- In Bayesian inference, we want to estimate the parameter, $\theta$, to be a random variable, with a prior distribution, $\pi(\theta)$
- We use data, $x$, to improve our estimate of $\theta$, $$\pi(\theta|x) = \frac{f(x|\theta)\pi(\theta)}{m(x)}$$ where $m(x)=\int f(x|\theta)\pi(\theta)d\theta$
- Example:
Let $X_1,\dots,X_n\sim\text{Bernoulli}(p)$ and $Y=\sum_i X_i$ (binomial random variable).
Goal: Estimate $p$.
  - Assume $p\sim\text{Beta}(\alpha,\beta)$ is a prior distribution for $p$.
  That is, $$\pi(p) = \frac{p^{\alpha-1}(1-p)^{\beta-1}}{B(\alpha,\beta)}$$ where $B$ is the beta function.
  Then $$\pi(p|Y) \propto \underbrace{\binom{n}{Y}p^Y(1-p)^{n-Y}}_{\text{likelihood}} \times     \underbrace{\frac{p^{\alpha-1}(1-p)^{\beta-1}}{B(\alpha,\beta)}}_{\text{prior}} \propto p^{Y+\alpha-1}(1-p)^{n-Y+\beta-1}$$
  Then $$p|Y\sim\text{Beta}(Y+\alpha,n-Y+\beta)$$ so that $$\hat p = \frac{Y+\alpha}{\alpha+\beta+n}$$ using the mean of a [Beta distributed](https://en.wikipedia.org/wiki/Beta_distribution) random variable
- Penalized regression was introduced as a frequentist method in 1996
- [Park and Casella](https://people.eecs.berkeley.edu/~jordan/courses/260-spring09/other-readings/park-casella.pdf) showed that it was equivalent to a Bayesian paradigm where parameters have a mean-zero Gaussian prior (ridge regression) or a meanzero-Laplace prior (lasso)
- Lasso as an optimization: $$\beta_{\text{Lasso}} = \arg\min_\beta \|y-X\beta\|^2 + \lambda\sum_{j=1}^p |\beta_j|$$
- Lasso as Bayesian: $y\sim\text{Gaussian}(X\beta, \sigma^2I_n)$ with prior, $\beta_j\sim \frac{\lambda}{2\sigma}e^{-\lambda|\beta_j|/\sigma}$ (Laplace distribution)
- Ridge as optimization: $$\beta_{\text{Ridge}} = \arg\min_\beta \|y-X\beta\|^2 + \lambda\sum_{j=1}^p \beta_j^2$$
- Ridge as Bayesian: $y\sim\text{Gaussian}(X\beta,\sigma^2I_n)$ with prior, $\beta_j\sim \text{Gaussian}(0, \lambda^{-1})$ (Gaussian distribution)
$$
\begin{align}
\text{Posterior Distribution}
&=\underbrace{\prod_{i=1}^n \text{Gaussian}(y_n|X\beta, \sigma^2I_n)}_{\text{likelihood}} \times \underbrace{\text{Gaussian}(\beta|0, \lambda^{-1}I_p)}_{\text{prior}}\\
&\propto \exp\left(-\sum_{i=1}^n \frac{(y_i-X_i\beta)^2}{\sigma}\right) \times \exp\left(-\lambda\sum_{j=1}^p \beta_j^2\right) \\
&= \exp\left(-\sum_{i=1}^n \frac{(y_i-X_i\beta)^2}{\sigma} - \lambda\sum_{j=1}^p \beta_j^2\right) \sim \text{Gaussian}
\end{align}
$$
- Using standard methods to optimize this over $\beta$, we get a similar result to $\beta_\text{Ridge}$ ($\lambda$ must be scaled to accomodate $\sigma$)

### Overfitting and Model Validation

![[image from here](https://www.quora.com/What-are-the-key-trade-offs-between-overfitting-and-underfitting)](./overfitting.png)

- Data generation paradigm: $Y = f(X) + \epsilon$ = signal + random noise
- prediction goal: estimate $f$ with a model $\hat f$
- Overfitting is including random noise into a prediction model
- Complex models are more likely to overfit
- Simple models are more likely to underfit (miss signal)
- Issue: how much complexity is too much?
- Question: Does a large $\lambda$ correspond more or less complexity?
- How do we assess fit?

**Validation**

- If a model has good fit, it should be able to make accurate predictions on new data
- Held out validation: 
  1. Randomly split data into training set and test set (why random?)
  2. Fit many different models on the training set
  3. Use each model to make predictions on test set
  4. Evaluate predictions and choose simplest model with most accurate predictions
  
```{r heldout}
# this code is for learning
# don't do it this way in practice
set.seed(1234)
# step 1
test_ind <- sample(1:sample_size, 10, replace=FALSE)
test_ind
train_ind <- (1:sample_size)[! 1:sample_size %in% test_ind]
train_ind

# step 2
lambdas <- 10^(seq(-2, 1, by=0.25))
train_fit <- glmnet(design_mat[train_ind,], y[train_ind], family="gaussian", alpha=0.5, lambda=lambdas)

# step 3
test_preds <- predict(train_fit, newx=design_mat[test_ind,], type='response')
test_preds

# step 4
# calculating mean squared error for each model
mses <- apply(test_preds, 2, function(x) mean((x-y[test_ind])^2))
cbind(mses, train_fit$lambda) # showing MSE with lambda value
plot(log10(train_fit$lambda), mses, type='b')
coef(train_fit, s=0.56234133) # coefficients for chosen model

# Comparing to the real betas
true_beta
```

**Cross Validation**

![[image from here](https://www.kaggle.com/alexisbcook/cross-validation)](./crossvalidation.png)

- Normally used over held out validation
- Runs held out validation $k$-fold times
Steps:
1. Partition data into $k$ folds
2. Run held out validation $k$ times:
  i. On step $j$, train model on all folds with the $j$th fold removed
  ii. Run predictions with trained model on $j$th fold
  iii. Save predictions
3. Evaluate model using loss function

- glmnet does all of this for you

```{r glmnetcv}
cv_fit0 <- cv.glmnet(design_mat, y, family="gaussian", alpha=0, nfolds=5)
cv_fit1 <- cv.glmnet(design_mat, y, family="gaussian", alpha=1, nfolds=5)
cv_fit5 <- cv.glmnet(design_mat, y, family="gaussian", alpha=0.5, nfolds=5)

par(mfrow=c(1,3))
plot(cv_fit0, sub='Ridge')
plot(cv_fit1, sub='Lasso')
plot(cv_fit5, sub='Elastic Net')
```

- Number above plot indicates number of nonzero coefficients at current $\lambda$

**Gene data with penalized regression**

Which penalized regression model should we use?

```{r elasticnetGenedat}
gene_mat <- as.matrix(shipp$x)
gene_fit <- glmnet(gene_mat, shipp$y, family="binomial", alpha=0.5)
plot(gene_fit, xvar='lambda', label=TRUE)

gene_fit_cv <- cv.glmnet(gene_mat, shipp$y, family="binomial", alpha=0.5, nfolds=5)
plot(gene_fit_cv)
```

