---
title: "Causal Inference"
author: "Octavio Mesner"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(twang)
library(reshape2)
```

## Association vs Causation

- What is causation?  Why is it important?
- Causal questions:
  - Which cancer treatments are best for which patients?
  - Will a national gun law result in fewer homicides?
  - Does increasing minimum wage reduce job openings?
  - Will changing `X` also change `Y`?

## Simpson's paradox

- How can statistics be misleading on causation?
- New infectious disease with high mortality rate
- Scientists develop an experimental treatment and give it to doctors to try out


```{r simpson paradox, echo=FALSE}
set.seed(1234)
size <- 1000
symptoms <- sample(c(0,1), size, replace = TRUE)
treatment <- rbinom(size, 1, 0.3 + 0.4*symptoms)
mortality <- rbinom(size, 1, 0.15 + 0.3*symptoms - 0.1*treatment)

symptoms <- ifelse(symptoms==1, 'severe', 'mild')
treatment <- ifelse(treatment==1, 'experimental', 'placebo')
mortality <- ifelse(mortality==1, 'died', 'alive')

df <- data.frame(treatment, symptoms, mortality)[sample(size),]
head(df, 20)
table(df$treatment)
table(df$symptoms)
table(df$mortality)
```

- The mortality rate for each group is below:
```{r mortalityTreatment}
table(df$treatment, df$mortality)[,2]/table(df$treatment)
```

- The death rate is higher in the experimental treatment group than the placebo group
- Taking a closer look, we stratify death rate by symptom severity:

```{r stratified}
table(df$treatment, df$symptoms, df$mortality)[,,2]/table(df$treatment, df$symptoms)
```

- Someone with severe symptoms is much more likely to die than someone with mild symptoms
- But, in both groups, the experimental treatment was associated with fewer deaths
- How can this happen?

```{r treatment}
table(df$treatment, df$symptoms)
chisq.test(df$treatment, df$symptoms)
```

- Those with severe symptoms were more likely to be on the experimental treatment
- Timing matters here!  Was severity taken before or after treatment?
- Neither scenario below is conclusive from the data, but knowing the time ordering can help rule one out

```{dot benefit, out.width = "30%", echo=FALSE}
digraph G {
  Severity -> Treatment [ label = "?"];
  Treatment -> Severity [ label = "?"];
  Severity -> Mortality
  Treatment -> Mortality [constraint=false]
}
```


## Confounding

- Example: Sleeping with shoes on is associated with waking up with a headache
- Example: Yellow teeth and cancer are confounded by smoking
- [Confounding](https://en.wikipedia.org/wiki/Confounding): `X` and `Y` are confounded if they are both influenced by a third variable
- Example from last section: Assume that severity was taken before treatment was given

```{dot benefit2, out.width = "30%", echo=FALSE}
digraph G {
  Severity -> Treatment
  Severity -> Mortality
  Treatment -> Mortality [constraint=false]
}
```

- There are two "open" paths from treatment to mortality:
  - `treatment <- severity -> mortality`
  - `treatment -> mortality`
- Looking at the total association between treatment and mortality (without severity) will use include assocations from both paths
- Controlling for severity "blocks" the `treatment <- severity -> mortality` pathway
- We're left with the direct causal path `treatment -> mortality`

### Confounding Simulations

```{dot confounding1, out.width = "30%", echo=FALSE}
digraph G {
  C -> X [ label = "2"];
  C -> Y [ label = "3"]
}
```

- Diagram above: `A` and `B` are confounded by `C` and have no direct causal relationship
- $X = 2C + \epsilon_X \Rightarrow C = 0.5X + 0.5\epsilon_X$, so $Y = 0.5 \cdot 3X + \text{noise}$

```{r confounding1r}
size <- 500
C <- 10*runif(size)
X <- 2*C + rnorm(size)
Y <- 3*C + rnorm(size)

summary(lm(Y~X))
summary(lm(Y~X+C))
```

```{dot confounding2, out.width = "30%", echo=FALSE}
digraph G {
  C -> X [ label = "2"];
  C -> Y [ label = "3"];
  X -> Y [ label = "5", constraint=false]
}
```

- Diagram above: `X` influences `Y` but the effect is confounded by `Z`.  That is, failing to account for `Z` in a regression will lead to an incorrect causal parameter estimate
- parameter = (counfounding bias) + (causal effect) = 1.5 + 5 = 6.5

```{r confounding2r}
Y <- 3*C + 5*X + rnorm(size)

summary(lm(Y~X))
summary(lm(Y~X+C))
```

- Note: the covariate causal effect math only works here because the simulated data is linear. In the real world, we typically can't assume linearity.  This math doesn't work without if the true data aren't linear.
- In most real-world datasets, there will always be the possibility of latent confounding

## Sampling Bias

- [Sampling Bias](https://en.wikipedia.org/wiki/Sampling_bias) occurs when sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.
- Sampling bias can lead to incorrect estimates when variables of interest influence sampling
- Example: In 1936, the American Literary Digest sent out two million surveys to its readers and predicted that Alf Landon would beat incumbent president, Franklin Roosevelt, by a landslide, but the opposite happened.  This was because readers over-represented Republicans.
- Example: good looking people are jerks (note: this is example is made up but hopefully memorable)

```{r sampleBias}
size <- 100
looks <- 10*runif(size)
nice <- 10*runif(size)
relationship <- ifelse(rbinom(size, 1, (looks+nice)/20)==1, 'in relationship', 'single')
df_all <- data.frame(looks, nice, relationship)
library(ggplot2)
ggplot(df_all[relationship=='single',], aes(nice, looks)) + geom_point() + 
  geom_smooth(method = lm, se = FALSE)
```

```{r noSampBias}
ggplot(df_all, aes(nice, looks)) + geom_point(aes(color=relationship)) +
  stat_smooth(method=lm, se=FALSE)
```

- Sampling bias is an example of collider bias
- Here we are conditioning on relationship status
- Even though looks and jerk are independent, conditioning on a collider induces an association
- Conditioning on a collider "open" an association pathway

```{dot sampleCollider, out.width = "30%", echo=FALSE}
digraph G {
  Looks -> Relationship 
  Nice -> Relationship
  Relationship -> Sampling
}
```

## Prediction, GLMs, and Causal Inference

- So far in this topic, we have focused on inference but not prediction
- The goals are usually for GLMs and causal inference models are similar in practice:
  - control for variables to approximate the causal influence of variables of interest
  - if model assumptions are satisfied (linearity, no latent confounding, etc), standard GLMs will give causal parameter estimates
  - in this case, causal inference models *should* give similar parameter estimates to GLM estimates
  - In both cases, we should only control for variables that can *cause* our outcome
    - That is, we should not control for variables that are caused by the outcome (this can lead to collider bias)
    - Some of the time, we will not know which variables are caused by the outcome, a general rule is to not control for variable that are taken or observed after the outcome occurs
- If we do not control for *all* confounding variables, any association between $X$ and $Y$ present through the confounder pathway (red in image below) will likely be included in the parameter quantified by a model
- In *any* inference, we want to quantify the true causal parameter
  - Causal inference models make this easier by relaxing some assumptions
    
```{dot regressionConf, out.width = "40%", echo=FALSE}
digraph G {
  Confounders -> X [color="red"];
  Confounders -> Y [color="red"];
  X -> Y [ label = "True Causal Parameter", constraint=false]
}
```
    
- Causal inference
  - Assumption of linear relationships within the data are likely not true (or at least hard to verify)
  - Causal inference models make it possible to control for confounding variables in a non-linear setting
  - Goal: Estimate a causal parameter of interest (when there are not linear relationships)
    - Causal parameters in our simulations are the parameters in the Bayesian network (DAG)
    - Causal parameter interpretation: if we were to change $X$, we should be able to use the causal parameter to accurately predict $Y$
    - Recall: when we did not control for confounding variables, the parameter estimates did not match the DAG
  - Assumptions: no latent confounding (we have all confounding variables included in the data)
- Prediction
  - use all useful information in data to make a prediction
  - we can use causes and effects
  - we also want to use colliders because they also provide information
  - General rule: use [Markov Blanket](https://en.wikipedia.org/wiki/Markov_blanket)
  - For prediction, all variables in Markov blanket are useful for prediction, but not causation
    - If we changes causes, we should see changes in outcome
    - If we change variables in collider, we should not
  - Note: Prediction methods try to find all variables in Markov blanket
    - This will depend on signal/noise ratio and model assumptions
  - Given Markov Blanket, all other variables are conditionally independent from outcome
  - If we *know* causal Bayesian network underlying a dataset, we should include all variables in Markov blanket and no others
    - Including variables not in Markov Blanket will be noise variables (given the Markov Blanket)
  
![Markov Blanket Image from Wikipedia](https://upload.wikimedia.org/wikipedia/commons/e/eb/Diagram_of_a_Markov_blanket.svg)

In the image above, the Markov Blanket of $A$ is the set of white variable within the large circle

## Graphical Causal Models and D-separation

- Researchers frequently use *directed acyclic graphs* (DAG) to visualize causal relationships between variables in a dataset
  - directed - all edges in graph have direction (all edges are arrows)
  - acyclic - arrow directions do not create a loop
  - DAGs are sometimes called Bayesian networks
- DAG below
  - A, B, C, D, E, F, G, H, I represent variables in a dataset
  - Arrows indicate a direct causal relationship
  - A is the *parent* of D
  - D is the *child* of A
  - B is the *ancestor* of I
  - I is the *successor* of B

```{dot networkex, out.width = "50%", echo=FALSE}
digraph G {
  A -> D
  A -> E
  B -> E
  C -> G
  D -> H
  E -> G
  F -> G
  F -> I
  G -> H
  G -> I
}
```

- Here: we are interested in paths between variables
  - Note: paths can have arrows pointing either direction
  - One path between D and F is
  \[D\leftarrow A\rightarrow E \rightarrow G \leftarrow F\]
  - Another is 
  \[D \rightarrow H \leftarrow G \rightarrow I \leftarrow F\]
  - These are of interest if we want to know if D and F are associated or causally linked
- Somewhat technical assumptions: in general assume arrows between nodes indicate a causal relationship
- Text on [Probabilistic Graphical Models](https://mitpress.mit.edu/books/probabilistic-graphical-models) for more detail
  
**Factorization and conditional independence**

- In causal inference we frequently want to isolate the average causal effect between two variables given some knowledge of the surrounding Bayesian network, i.e. potential confounders or colliders
- In reality we rarely know the Bayesian structure of a dataset, but these concepts help inform causal inference.

```{dot simplenetwork, out.width = "30%", echo=FALSE}
digraph G{
  A -> B
  B -> D
  C -> D
  C -> E
}
```

- Joint distribution can factored as
  \[P(A, B, C, D, E) = P(A) P(B|A) P(C) P(D|B,C) P(E|C)\]
- DAGs give information about conditional independence relationships among variables
- This DAG has one path from A to E: $A \rightarrow B \rightarrow D \leftarrow C \rightarrow E$
- How can we use this DAG to determine statistical associations between variables/nodes?
- With 3 nodes:
  - Chain: $A \rightarrow B \rightarrow C$
  - Other Chain: $A \leftarrow B \leftarrow C$
  - Fork: $A \leftarrow B \rightarrow C$
  - Collider: $A \rightarrow B \leftarrow C$


| Structure | Path type | $A,C$ independent/dependent?  | $A,C$ conditionally independent/dependent given $B$ |
| --- | --- | --- | --- |
| Chain | $A \rightarrow B \rightarrow C$  | $A\not\perp C$ | $A\perp C|B$ |
| Other Chain | $A \leftarrow B \leftarrow C$ | $A\not\perp C$  | $A\perp C|B$ |
| Fork (Confounder) | $A \leftarrow B \rightarrow C$ |  $A\not\perp C$ |  $A\perp C|B$ |
| Collider | $A \rightarrow B \leftarrow C$ | $A\perp C$ | $A\not\perp C|B$ |

  
- Questions:
  1. Are A and B associated?
  2. Are A and D associated?
  3. Are D and E associated?
  4. Are B and C associated?
  5. Are A and E associated?
 
 Rule: If a path has no colliders, then yes and we say that the path is open.
 If there is a collider on the path, then no and we say that the path is blocked
 
- What out conditioning?
- Questions:
  6. Are A and D associated given B?
  7. Are D and E associated given C?
  8. Are B and C associated given D?
  9. Are A and C associated given D?
  10. Are A and E associated given D?
  11. Are A and E associated given B?
  12. Are A and E associated given B, D?
  
  Rule: Conditioning on a collider opens a path.
  Conditioning on a non-collider blocks a path
  
- Note: A variable/node and be a collider on one path but not on another.

How can we show that if $D \leftarrow C \rightarrow E$ then $D \perp E | C$?

- $D \perp E | C$ if and only if $P(D,E | C) = P(D|C) P(E|C)$
\[P(D,E|C) = \frac{P(D,E,C)}{P(C)} = \frac{P(C)P(D|C)P(E|C)}{P(C)} = P(D|C)P(E|C)\]

What about $A \rightarrow B \rightarrow C$?

**Markov Assumption**

```{dot markov, out.width = "20%", echo=FALSE}
digraph G{
  Study -> Sleep
  Study -> Score
  Sleep -> Score
}
```

- We are implicitly using the Markov assumption here
- Specifically, we are assuming different paths won't perfectly cancel out
  - I want a good score on my test so I continue studying
  - With the additional studying, I sleep less
  - With less sleep, my performance isn't as good
  - With the extra studying, I know the material better
  - Can my lack of sleep and extra studying perfectly cancel out?
  - Markov assumption says no
  
**Regression with knowledge of causal structure**

```{dot regnetwork, out.width = "30%", echo=FALSE}
digraph G{
  A -> M1
  M1 -> M2
  M2 -> Y
  A -> Y
  C1 -> C2
  C1 -> C3
  C2 -> A
  C3 -> Y
  A -> D1
  D1 -> D2
  Y -> D3
  D3 -> D2
}
```

- We want to isolate the causal effect of A on Y
- Assume there are no other relevant variable other than what is in the DAG
- Which variables need to be taken into account and which should be avoided?

```{dot hardexample, out.width = "30%", echo=FALSE}
digraph G{
  A -> Y
  C1 -> A
  C1 -> Y
  C2 -> A
  C3 -> Y
  C2 -> C1
  C3 -> C1
}
```
 
 
## Evaluating Work Training Programs

- Manpower Demonstration Research Corporation was a federally and privately funded program implemented in the mid-1970s to provide work experience for a period of 6-18 months to individuals who faced economic and social problems prior to enrollment
- Those selected to join the program participated in various types of work such as restaurant and construction
- Pre-treatment information was collected - earnings, education, age, ethnicity, marital status
- All observations here are from men
- See [Dehejia, R.H. and Wahba, S. (1999). Causal Effects in Nonexperimental Studies: Re-Evaluating the Evaluation of Training Programs. Journal of the American Statistical Association 94: 1053-1062](https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10473858?casa_token=dsAisSiC-v4AAAAA:Auzr8KHp8-iB9Gy3T5o9hL-usKjKR1rne_TUvZDkHUCcI31OlVk_c0vwikXNTwQVYIKhhgSKqKXRJw)

```{r ses}
data(lalonde)
dim(lalonde)
names(lalonde)
table(lalonde$treat)
lalonde$treat <- ifelse(lalonde$treat == 1, TRUE, FALSE)

#age
tapply(lalonde$age, lalonde$treat, mean)
t.test(lalonde$age ~ lalonde$treat)
ggplot(lalonde, aes(x=age, fill=treat)) +  geom_density(alpha=0.25)

#educ
tapply(lalonde$educ, lalonde$treat, mean)
t.test(lalonde$educ ~ lalonde$treat)
ggplot(lalonde, aes(x=educ, fill=treat)) +  geom_density(alpha=0.25)

#black
table(lalonde$black, lalonde$treat)
chisq.test(lalonde$black, lalonde$treat)
ggplot(lalonde, aes(x=black, fill=treat)) +  geom_bar(position = 'dodge')

#hispan
table(lalonde$hispan, lalonde$treat)
chisq.test(lalonde$hispan, lalonde$treat)
ggplot(lalonde, aes(x=hispan, fill=treat)) +  geom_bar(position = 'dodge')

#married
table(lalonde$married, lalonde$treat)
chisq.test(lalonde$married, lalonde$treat)
ggplot(lalonde, aes(x=married, fill=treat)) +  geom_bar(position = 'dodge')

#nodegree
table(lalonde$nodegree, lalonde$treat)
chisq.test(lalonde$nodegree, lalonde$treat)
ggplot(lalonde, aes(x=nodegree, fill=treat)) +  geom_bar(position = 'dodge')

#re74
tapply(lalonde$re74, lalonde$treat, mean)
t.test(lalonde$re74 ~ lalonde$treat)
ggplot(lalonde, aes(x=re74, fill=treat)) +  geom_density(alpha=0.25)

#re75
tapply(lalonde$re75, lalonde$treat, mean)
t.test(lalonde$re75 ~ lalonde$treat)
ggplot(lalonde, aes(x=re75, fill=treat)) +  geom_density(alpha=0.25)
```


## Quantifying Causal Effect with Counterfactuals
- Let $Y_i^1$ be the outcome under treatment for observation $i$ and let $Y_i^0$ be the outcome without treatment for observation $i$.
- Example: Does taking vitamin C prevent sickness?  
  - $Y_i^1 = 1$ if $i$ takes vitamin C and stays healthy
  - $Y_i^1 = 0$ if $i$ takes vitamin C and gets sick
   - $Y_i^0 = 1$ if $i$ does not take vitamin C and stays healthy
  - $Y_i^0 = 0$ if $i$ does not take vitamin C and gets sick
- The causal effect for observation $i$ is
\[Y_i^1 - Y_i^0\]
- Unfortunately, it's not possible to any individual's with and without treatment

```{r counterfactual data, echo=FALSE}
set.seed(1234)
A <- c(rep(0,4), rep(1,4))
Y0 <- rbinom(4, 1, 0.3)
Y1 <- rbinom(4, 1, 0.7)
Y <- c(Y0, Y1)
Y0 <- c(Y0, rep(NA,4))
Y1 <- c(rep(NA,4), Y1)
data <-data.frame(A,Y, Y0, Y1)
data
```

- In a population, the average causal effect (ATE) is
\[\text{ATE} = E\left[Y^1 - Y^0\right] = E[Y^1] - E[Y^0]\]
- Estimated as
\[\widehat{\text{ATE}} = \frac{1}{n} \sum_{i=1}^n\left(Y_i^1 - Y_i^0\right)\]
- Is $E(Y^1|A=1)$ different from $E(Y|A=1)$?
- $Y^1 = Y^{A=1}$ and $Y^0 = Y^{A=0}$ assumes that $A$ is not influenced by any variables, measured or latent

```{dot counterfactual, out.width = "30%", echo=FALSE}
digraph G {
  C -> A [color="lightgray"];
  C -> Y [];
  A -> Y [ label = "ATE", constraint=false]
}
```
No other variables influence $A$ when we consider $Y^1, Y^0$

- ATE can be interpreted as the average difference in outcome within the population that is attributed to $A$
- There may be other reasons $Y$ differs from person to person, like age, severity, etc
- Conditional Average Causal Effect (CATE): if $Z$ is another covariate,
\[\text{CATE}_z = E[Y^1|Z=z] - E[Y^0|Z=z]\]
is the average causal effect for group $Z=z$.

## Randomized Controlled Experiments or Trials
- When does a parameter estimate have a causal interpretation?
- Vaccine trails: a population is randomized to receive a test vaccine or placebo
  - Single or double blind: participants (and sometimes researcher) are not told which group they are in
  - Because of randomization, confounding is not possible and with a large enough sample, the two group will be statistically identical other than vaccine treatment
  - Any difference in infection acquisition can be attributed to vaccination.
  - Can estimate ATE:
    \[\begin{align}
    \widehat{\text{ATE}}
    &= E[Y^{\text{vaccine}}] - E[Y^{\text{placebo}}] \\
    &= \frac{1}{n} \sum_{i=1}^n Y_i^{\text{vaccine}} - \frac{1}{m} \sum_{j=1}^m Y_j^{\text{placebo}}
    \end{align}\]
  - $H_0: \widehat{\text{ATE}} = 0, H_1: \widehat{\text{ATE}} > 0$
  - Two sample t-test is sufficient
- Randomization: what would the treatment and control populations look like for the work training data if treatment were randomized?

```{r randomization}
set.seed(1234)
lalonde$random <- sample(rep(c(TRUE, FALSE), length.out=dim(lalonde)[1]))
ggplot(lalonde, aes(x=treat, fill=random)) + geom_bar(position = 'dodge')
ggplot(lalonde, aes(x=age, fill=random)) +  geom_density(alpha=0.25)
ggplot(lalonde, aes(x=black, fill=random)) + geom_bar(position = 'dodge')
ggplot(lalonde, aes(x=hispan, fill=random)) + geom_bar(position = 'dodge')
ggplot(lalonde, aes(x=married, fill=random)) + geom_bar(position = 'dodge')
ggplot(lalonde, aes(x=nodegree, fill=random)) + geom_bar(position = 'dodge')
ggplot(lalonde, aes(x=re74, fill=random)) +  geom_density(alpha=0.25)
ggplot(lalonde, aes(x=re78, fill=random)) +  geom_density(alpha=0.25)
```

- Using natural randomization: In 1973 the Eldfell volcano in Iceland on the island of Heimaey erupted, destroying about 400 homes. The Icelandic government compensated those who lost their homes, many never returned. An [economics paper](https://www.nber.org/papers/w22392.pdf) showed that, among people less than 25 years old at the time of the eruption, those who had moved averaged four more years of schooling and earnings $27,000 greater per year than those from families who had kept their home.
  - Because those who lost their homes was naturally randomized, this paper used instrumental variables
  - The treatment was moving away and the outcome was later earnings
  - Losing home -> moving away -> later earnings
  - We are not going to focus on instrumental variables analysis here
  - Used when there is a natural experiement
  
## Inverse Probability Treatment Weighting (treatment not randomized)

- In observational data, the population receiving a treatment is difficult to compare to the other groups because of possible confounding
  - In the first example, the treatment and non-treatment populations had different severity levels, so are hard to compare directly
  ```{r treatment2}
  table(df$treatment, df$symptoms)
  chisq.test(df$treatment, df$symptoms)
  ```
  - For comparison, we want the treatment and placebo populations to be generated from the same distribution
  - This breaks any association between confounders and treatment
- IPTW uses (possible) confounders to up- or down-weight observations depending on their probability of  receiving treatment
  - IPTW are sometimes called propensity scores (PS)
  - These are called propensity scores - they indicate propensity for an observation to be in the the treatment group
  - IPTW makes the populations look similar for the considered covariates
  - Side note: Sometimes people are matched on covariates.  What is a possible draw back of matching?  What about many covariates
- Why not use regression to control for confounders [McCaffrey et all 2013](https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5753?casa_token=-_Iuef6qRG0AAAAA:jACp_crwgA9QTkHCNfDi-4lJuMDbCXkHDocwRzvvnky5S6rPDQ-j-Cn8QtXNvK5eCKz0ziJ5cURXTw):
  1. By summarizing all pretreatment variables to a single score, propensity scores are an important dimension reduction tool for evaluating treatment effects. This characteristic of propensity scores is particularly advantageous over standard adjustment methods when there exists a potentially large number of pretreatment covariates.
  2. Propensity score methods derive from a formal model for causal inference, the potential outcomes framework, so that causal questions can be well defined and explicitly specified and not conflated with the modeling approach as they are with traditional regression approaches.
  3. Propensity score methods do not require modeling the mean for the outcome. This can help avoid bias from misspecification of that model.
  4. Propensity score methods avoid extrapolating beyond the observed data unlike parametric regression modeling for outcomes, which extrapolate whenever the treatment and control groups are disparate on pretreatment variables.
  5. Propensity score adjustments can be implemented using only the pretreatment covariates and treatment assignments of study participants without any use of the outcomes. This feature of propensity score adjustments is valuable because it eliminates the potential for the choice of model specification for pretreatment variables to be influenced by its impact on the estimated treatment effect.
- Assumptions:
  1. Sufficient overlap: For all $i$
  \[0 < P(A=1|C=x_i) < 1\]
  2.  No unknown confounders: $A \perp Y^t | X$ for $t=0,1$
  
- IPW: For each observation, $i$, let
$$p_i = P(A=1|C=x_i) = P(i \text{ gets treatment}|x_i)$$

the associated weight is
\[w_i = \begin{cases}
\frac{1}{p_i} & \text{when }A=1 \\
\frac{1}{1-p_i} & \text{when }A=0
\end{cases}\]

- Propensity score Theorem
  \[(Y^0,Y^1) \perp A | X \Rightarrow (Y^0, Y^1) \perp A | w(X)\]
- Note: This is saying that getting treatment or not is independent of what the response would have been in a counterfactual setting
- Once the weights are estimates, ATE for a binary treatment is estimated as
$$\widehat{\text{ATE}} = \frac{\sum_{i=1}^n A_iY_iw_i}{\sum_{i=1}^n A_iw_i} -\frac{\sum_{i=1}^n (1-A_i)Y_iw_i}{\sum_{i=1}^n (1-A_i)w_i}$$

## Estimating Weights

- There is no one standard method for model selection in the context of estimating propensity scores for IPTW for multiple treatments
- It's common to use non-parametric methods for estimating probabilities
- Note that in this context, we care more about the prediction value than the interpretation of parameter estimates
- Let's compare logistic regression and generalized boosted models (GBM) to estimate observation weights

**Logistic Regression**
```{r logisticPropensity}
prop.mod <- glm(treat ~ age + educ+  black + hispan + married + nodegree + re74 + re75, data=lalonde, family=binomial())
summary(prop.mod)
treat_prop_logistic <- predict(prop.mod, newdata = lalonde[,-1], type = 'response')
lalonde$logistic_prob <- treat_prop_logistic
lalonde$logistic_weight <- ifelse(lalonde$treat, 1/(1-treat_prop_logistic), 1/treat_prop_logistic)
head(lalonde)
hist(lalonde$logistic_weight)
boxplot(logistic_prob ~ treat, data=lalonde)
```

- Assessing covariate balance after weighting
\[PSB_k = \frac{| \bar X_{k1} - \bar X_{k0}|}{\widehat \sigma_k}\]
where $\bar X_{kt} = \sum_{i=1}^n I(A=t) X_{ki} w_i / \sum_{i=1}^n I(A=t) w_i$

**Boosting**

- A problem with logistic regression for estimating weights is that can be challenging to get a good model fit
- Boosting makes this a lot easier and automatic
- We already talked about AdaBoost and a little about general boosting
- For a reference, in AdaBoost
  - each lazy learner is fitted to the data with weights that depend on the last model's performance
  - all of the lazy learners are scored depending on their performance
- For general boosting
  - Observations are *not* weighted
  - We use learners (models) like regression or trees that are more complex than lazy learner
  - Similar to AdaBoost, we must choose the number of sequential learner to use, $M$
  - Each learner tries to predict the *error* of the previous model
  - Error: in the literature, the error is the negative gradient of the loss function with respect to the model evaluated at an observation: \[-\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\]
  - For a continuous outcomes, this is $y_i-f(x_i)$ for continuous outcomes where $f$ is a continuous learner
  - For a binary outcomes, this is $I(y_i=1)-p(x_i)$ where $p$ is a learner like logistic regression model or decision tree
  - For categorical outcomes, this is $I(y_i=\text{Category }k)-p_k(x_i)$ where $p_k$ is models the probability of category $k$
  - Learning rate: to avoid over fitting, we reduce the contribution of each learner by $0<\nu<1$
  - Trees are the most common learner to use for boosting, usually with between 8 and 32 terminal leaves
  
  **Gradient Tree Boosting Algorithm from ESL**
  
1. Initialize $f_0(x)=\arg\min_\gamma \sum_{i=1}^n L(y_i,\gamma)$
2. For $m=1$ to $M$:
    1. For $i=1,\dots,n$ compute the error $$r_{im}=-\left[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\right]_{f=f_{m-1}}$$
    2. Fit a regression tree to the errors, $r_{im}$ with terminal regions $R_{jm}$ where $j=1,\dots, J_m$
    3. For $j=1,\dots, J_m$ compute $$\gamma_{jm}=\arg\min_\gamma \sum_{x_i\in R_{jm}} L(y_i,f_{m-1}(x_i)+\gamma)$$
    4. Update $f_m(x)=f_{m-1}(x)+\nu\sum_{j=1}^{J_m} \gamma_{jm}I(x\in R_{jm})$
3. Output $\hat f(x)=f_M(x)$

**Generalized Boosting for Propensity Scores**

- For causal inference, we want to weight the observation to make the treatment and control groups look as if they were randomized
- Using inverse probabilities from boosting model, we want to use as many trees as we need to achieve balance in the covariates
- Ideally, we want the distributions of each covariate in the treatment and control groups to be similar (mean, variance, skew, shape, etc)
- Clearly this is rarely possible, so we might restrict balance to mean and/or standard deviation for example
- Assessing covariate balance with *standardize bias* estimation
\[SB_k = \frac{|\bar X_{k1} - \bar X_{k0}|}{\hat \sigma_k}\]
- Generally, standardized mean differences of less than 0.20are considered small, 0.40 are considered moderate, and 0.60 are considered large
- This cutoff can change within fields and between investigator
- McCaffery et al: $SB>0.2$ is problematic
- Below the `twang` R package automatically add mores trees until a stopping rule based on balance it met
- `twang` chooses propensity scores based on the boosted model with the best balance
- see [twang documentation](https://cran.r-project.org/web/packages/twang/twang.pdf)

```{r boosted}
boosted.mod <- ps(treat ~ age + educ + black + hispan + married + nodegree + re74 + re75,
                  data=lalonde,
                  estimand = "ATE",
                  n.trees = 5000, 
                  interaction.depth=2, 
                  perm.test.iters=0, 
                  verbose=FALSE, 
                  stop.method = c("es.mean"))
summary(boosted.mod)
summary(boosted.mod$gbm.obj,
        n.trees=boosted.mod$desc$es.mean.ATE$n.trees, 
        plot=FALSE)
lalonde$boosted <- get.weights(boosted.mod)
hist(lalonde$boosted)
plot(boosted.mod)
plot(boosted.mod, plots=2)
plot(boosted.mod, plots=3)
bal.table(boosted.mod)
```

## Estimating Average Treatment Effect (ATE)
- Want $E[Y^1] - E[Y^0]$
- Treatment population mean estimate for $t=0,1$:
\[\widehat\mu_t = \frac{\sum_{i=1}^n I(T_i=t) Y_i w_i(t)}{\sum_{i=1}^n I(T_i=t) w_i(t)}\]
- Estimate $E[Y^1] - E[Y^0]$ as $$\widehat{\text{ATE}} = \widehat\mu_1-\widehat\mu_0$$
- We can use a weighted *t-test* to evaluate $\widehat{\text{ATE}}$
- McCaffery et all suggests using `svyglm` which achieves the same goal

```{r outcome}
library(survey)
design <- svydesign(ids=~1, weights=~boosted, data=lalonde)
glm1 <- svyglm(re78 ~ treat, design=design)
summary(glm1)

summary(lm(re78 ~ treat + age + educ+  black + hispan + married + nodegree + re74 + re75, data=lalonde))
```

- interpretation: causal vs statistical
- Limitation: Umeasured confounding
```{dot unmeasured, out.width = "30%", echo=FALSE}
digraph G {
  X -> A
  X -> Y
  U -> A
  U -> Y
  A -> Y [constraint=false]
}
```


## Writing tips

- Intro paragraph:
  - Give the big picture in one sentence.  What is the general application field?
  - What is the problem you are attempting to answer?
  - How will you answer it?
  - In one sentence, what did you find?
- Give the baseline characteristics of the data after cleaning
  - How many observations with salient brake down
  - A table is a very efficient way to summarize data:
  
| | Total (n)       | Treatment (n1) | Control (n2)  | p-value |
| ---- | ------------- | ------------- | ----- | ---- |
| Covariate 1 | mean (sd)      | mean (sd) | mean (sd) | <0.001 |
| Covariate 2 | %  | %  |   % | 0.34 |
| Covariate 3 | median (q1, q3) |  median (q1, q3)   |  median (q1, q3) | 0.02 |

- Present you most important model (or models)
  - Clearly interpret the parameter of interest and its connection to the larger question that is being asked
  - A lot of this material is technical.  You're job is to understand the larger statistical picture and communicate it in an easy to understand way to someone who does not have any background in statistics an who probably does not do math regularly
- Example: how would you describe IPW to a client?
  1. Why do we use IPW in the first place?
  2. What does IPW do?
  3. How does IPW attempt to allow a causal interpretation for ATE?
- Conclusion:
  - Restate what you attempted to answer
  - State any potential limitation
  - Reiterate findings
- Don't
  - try to explain d-separation or Bayesian networks
  - think of a statistician as your audience
- Do
  - rely on intuition about causation
  - use your understanding of causal modeling to answer the question

